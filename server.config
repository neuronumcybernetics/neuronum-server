# ============================================================================
# Neuronum Server CONFIGURATION
# ============================================================================
# This file contains all configuration parameters for the Neuronum Server.
# Modify these values to customize the agent's behavior.

# --- Cell ---
# Note: The mnemonic is automatically loaded from ~/.neuronum/.env
# Make sure you've run 'neuronum connect-cell' or 'neuronum create-cell' first


# --- File Paths ---
LOG_FILE = "server.log"
DB_PATH = "agent_memory.db"
TASKS_DIR = "./tasks"

# --- Model Configuration ---
# Maximum tokens to generate in responses (business use: longer, complete answers)
MODEL_MAX_TOKENS = 512

# Temperature for sampling (0.0 = deterministic, 1.0 = creative)
# Lower temperature for business: more focused, consistent, and reliable responses
MODEL_TEMPERATURE = 0.3

# Nucleus sampling parameter (top-p)
# Lower top-p for business: more predictable and coherent outputs
MODEL_TOP_P = 0.85

# --- vLLM Server Configuration ---
# Model to load in vLLM server
# Examples: "Qwen/Qwen2.5-3B-Instruct", "Qwen/Qwen2.5-1.5B-Instruct", "meta-llama/Llama-3.2-3B-Instruct"
VLLM_MODEL_NAME = "Qwen/Qwen2.5-3B-Instruct"

# Server host (127.0.0.1 for local only, 0.0.0.0 to allow external connections)
VLLM_HOST = "127.0.0.1"

# Server port
VLLM_PORT = 8000

# Base URL for the vLLM API server (constructed from host and port)
VLLM_API_BASE = "http://127.0.0.1:8000/v1"

# --- Database/RAG Configuration ---
# Number of recent conversation messages to include in context
# Higher for business: better context retention for multi-turn conversations
CONVERSATION_HISTORY_LIMIT = 10

# Maximum number of knowledge chunks to retrieve from database
# Higher for business: more comprehensive knowledge retrieval
KNOWLEDGE_RETRIEVAL_LIMIT = 5

# Stop words to exclude from FTS5 search queries
FTS5_STOPWORDS = {"what","is","the","of","and","how","do","does","a","an","to","it","i","can","you"}
